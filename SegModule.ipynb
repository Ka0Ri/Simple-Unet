{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation for Lung CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "from skimage.transform import pyramid_reduce, resize\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "random.seed(17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LungCTscan(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.img_list = sorted(glob.glob(data_dir + '/2d_images/*.tif'))\n",
    "        self.mask_list = sorted(glob.glob(data_dir + '/2d_masks/*.tif'))\n",
    "        self.transform = transform\n",
    "        self.image_size = 256\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_list[idx]\n",
    "        mask_path = self.mask_list[idx]\n",
    "\n",
    "        # load image\n",
    "        image = imread(image_path) / 255.0\n",
    "        # resize image with 1 channel\n",
    "        image = resize(image, output_shape=(self.image_size, self.image_size), preserve_range=True)\n",
    "\n",
    "        # load image\n",
    "        mask = imread(mask_path) / 255.0\n",
    "        # resize mask with 1 channel\n",
    "        mask = resize(mask, output_shape=(self.image_size, self.image_size), preserve_range=True)\n",
    "        image, mask = np.array(image[..., np.newaxis], dtype=np.float32), np.array(mask[..., np.newaxis], dtype=np.float32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = LungCTscan(data_dir=\"CT-scan-dataset\")\n",
    "\n",
    "train_data, test_data = random_split(test_data, [200, 67])\n",
    "\n",
    "image, mask = test_data[0]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image.squeeze())\n",
    "ax[1].imshow(mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mask == 0).any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "def deconvrelu(in_channels, out_channels, kernel, stride, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel, stride=stride, padding=padding, output_padding=1),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Network description\n",
    "    \"\"\"\n",
    "    def __init__(self, n_filters = 16):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Contracting Path\n",
    "        self.conv1 = convrelu(1, n_filters*1, 3, 1)\n",
    "        self.conv2 = convrelu(n_filters*1, n_filters*2, 3, 1)\n",
    "        self.conv3 = convrelu(n_filters*2, n_filters*4, 3, 1)\n",
    "        self.conv4 = convrelu(n_filters*4, n_filters*8, 3, 1)\n",
    "        self.conv5 = convrelu(n_filters*8, n_filters*16, 3, 1)\n",
    "\n",
    "        # Expansive Path\n",
    "        self.conv_up6 = deconvrelu(n_filters*16, n_filters*8, 3, 2, 1)\n",
    "        self.conv6 = convrelu(2*n_filters*8, n_filters*8, 3, 1)\n",
    "        self.conv_up7 = deconvrelu(n_filters*8, n_filters*4, 3, 2, 1)\n",
    "        self.conv7 = convrelu(2*n_filters*4, n_filters*4, 3, 1)\n",
    "        self.conv_up8 = deconvrelu(n_filters*4, n_filters*2, 3, 2, 1)\n",
    "        self.conv8 = convrelu(2*n_filters*2, n_filters*2, 3, 1)\n",
    "        self.conv_up9 = deconvrelu(n_filters*2, n_filters*1, 3, 2, 1)\n",
    "        self.conv9 = convrelu(2*n_filters*1, n_filters*1, 3, 1)\n",
    "\n",
    "        self.out = nn.Conv2d(n_filters*1, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        c1 = self.conv1(x)\n",
    "        p1 = nn.MaxPool2d(2)(c1)\n",
    "\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = nn.MaxPool2d(2)(c2)\n",
    "\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = nn.MaxPool2d(2)(c3)\n",
    "\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = nn.MaxPool2d(2)(c4)\n",
    "        \n",
    "        c5 = self.conv5(p4)\n",
    "\n",
    "        u6 = self.conv_up6(c5)\n",
    "        cat6 = torch.cat([u6, c4], dim=1)\n",
    "        c6 = self.conv6(cat6)\n",
    "\n",
    "        u7 = self.conv_up7(c6)\n",
    "        cat7 = torch.cat([u7, c3], dim=1)\n",
    "        c7 = self.conv7(cat7)\n",
    "\n",
    "        u8 = self.conv_up8(c7)\n",
    "        cat8 = torch.cat([u8, c2], dim=1)\n",
    "        c8 = self.conv8(cat8)\n",
    "\n",
    "        u9 = self.conv_up9(c8)\n",
    "        cat9 = torch.cat([u9, c1], dim=1)\n",
    "        c9 = self.conv9(cat9)\n",
    "\n",
    "        output = self.out(c9)\n",
    "        # output = torch.sigmoid(output)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.MSE = nn.MSELoss(size_average=True)\n",
    "\n",
    "    def forward(self, labels, seg, bce_weight=0.5):\n",
    "        bce = F.binary_cross_entropy_with_logits(seg, labels)\n",
    "        # loss = self.MSE(labels, seg)\n",
    "     \n",
    "        return bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 0.0001,\n",
    "    \"max_epochs\": 100,\n",
    "    'lr_step': 10,\n",
    "    'lr_decay': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.types import File\n",
    "\n",
    "class UnetModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet()\n",
    "        self.loss = LossFunction()\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = y.squeeze()\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y, y_hat)\n",
    "        self.log(\"metrics/batch/loss\", loss, prog_bar=False)\n",
    "        \n",
    "        y_true = y.cpu().detach()\n",
    "        y_pred = y_hat.cpu().detach()\n",
    "        # print(y_pred)\n",
    "       \n",
    "\n",
    "        self.training_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.training_step_outputs\n",
    "        loss = np.array([])\n",
    "      \n",
    "        for results_dict in outputs:\n",
    "            loss = np.append(loss, results_dict[\"loss\"])\n",
    "          \n",
    "        self.log(\"metrics/epoch/loss\", loss.mean())\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # y = y.squeeze()\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y, y_hat)\n",
    "\n",
    "        y_true = y.cpu().detach()\n",
    "        y_pred = torch.sigmoid(y_hat).cpu().detach()\n",
    "        \n",
    "        self.validation_step_outputs.append({\"loss\": loss.item(), \"y_true\": y_true, \"y_pred\": y_pred})\n",
    "    \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        loss = np.array([])\n",
    "      \n",
    "        for results_dict in outputs:\n",
    "            loss = np.append(loss, results_dict[\"loss\"])\n",
    "       \n",
    "        y_true = make_grid(outputs[0][\"y_true\"], nrow=int(PARAMS[\"batch_size\"] ** 0.5))\n",
    "        y_pred = make_grid(outputs[0][\"y_pred\"], nrow=int(PARAMS[\"batch_size\"] ** 0.5))\n",
    "        y_true = y_true.cpu().numpy().transpose(1, 2, 0)\n",
    "        y_pred = y_pred.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        self.log(\"val/epoch/loss\", loss.mean())\n",
    "        self.logger.experiment[\"val/epoch/loss\"] = loss.mean()\n",
    "        self.logger.experiment[\"val/gt_images\"].append(File.as_image(y_true))\n",
    "        self.logger.experiment[\"val/outputs\"].append(File.as_image(y_pred))\n",
    "        # self.logger.experiment['val/epoch/outputs'] = y_pred\n",
    "       \n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=PARAMS['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=PARAMS['lr_step'], \n",
    "                                                    gamma=PARAMS['lr_decay'])\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"kaori/Seg\",\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vbmV3LXVpLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9uZXctdWkubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyZjZiMDA2YS02MDM3LTQxZjQtOTE4YS1jODZkMTJjNGJlMDYifQ==\",\n",
    "    log_model_checkpoints=False,\n",
    ")\n",
    "\n",
    "neptune_logger.log_hyperparams(params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "unet_model = UnetModule()\n",
    "\n",
    "Test_transform = transforms.Compose([\n",
    "            # transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),# default : range [0, 255] -> [0.0,1.0]\n",
    "            # transforms.Normalize((0.5), (0.5))\n",
    "        ])\n",
    "train_ds = LungCTscan(data_dir=\"CT-scan-dataset\", transform=Test_transform)\n",
    "train_data, val_data = random_split(train_ds, [200, 67])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=PARAMS[\"batch_size\"])\n",
    "val_loader = DataLoader(val_data, batch_size=PARAMS[\"batch_size\"])\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=os.path.join(\"model\", \"test\"), save_top_k=1, monitor='metrics/batch/loss', mode=\"min\")\n",
    "\n",
    "    # (neptune) initialize a trainer and pass neptune_logger\n",
    "trainer = Trainer(\n",
    "    logger=neptune_logger,\n",
    "    max_epochs=PARAMS[\"max_epochs\"],\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "#Training and save model\n",
    "trainer.fit(unet_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Validation\n",
    "trainer.validate(unet_model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
